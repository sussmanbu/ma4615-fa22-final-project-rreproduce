---
title: Data
description:
toc: true
featuredVideo:
featuredImage: images/data-import-cheatsheet-thumbs.png
draft: false
---

Data Link

We care about the social justice in the United States. We hope to find out how migration would impact the change of the crime rate in the United States between (year1) and (year 2). Additionally, we are interested in find out whether other variables would moderate the association between migration pattern and crime rate, such as income levels and race.
Our migration data is from the [XXX]. We assorted the data in to several different data set, including [XXX]. Specifically, we used [sub-dataset 1], which describes [XXX], and [sub-dataset 2], which describes [XXX] to construct our models. Our crime rate data is from the [XXX]. We assorted our data into the number of “rubbery” and “bulglary” cases during [Year] by the states in the US.
Here are the links of the two data sets that we used: [Link].

Variables

The migration dataset contains # variables. We used # variables listed below:
1.	resident number before migration by state (numeric)
2.	resident number after migration by state (numeric)
3.	original family income (categorical; from Q1 to Q5)
4.	Race (categorical; [describe the race])
5.	......

The crime rate dataset contains 3 variables.
1.	State (nominal; 50 in total)
2.	Mean number of rubbery cases by states (numeric) in (year)
3.	Mean number of bulglary cases by states in (year)

Specific Created Variables


##Data Import and Cleaning

After importing our primary dataset od.csv, we first separate the "pool" column into "race" and "income"

```{r}
## pool     ->    race    |   income
## WhiteQ1  ->   White    |     Q1
```

Next we filter out 0 values from 'n' column (indicating that the path has no immigrants) and -1 and NA values from all cells (invalid number)

```{r}
##   filter(n != 0 & n != -1) 
##   filter(n_tot_o != -1) 
##   filter(n_tot_d != -1)
##   od_data[is.na(od_data)] = 0
```


To explore the data structure and out interests in the dataset, we have also created a sub dataset that contains people moved from MA to other states/stayed in MA. (see o_MA)


Our secondary data is separated in multiple files range from different years and each row represents different information by state. 
Therefore, we only extracted the row indicating the total cases of crime categories of all states.
Crime categories do not have data for every year except for "Robbery" and "Burglary". For the consistency of the analysis, we only use case numbers of "Robbery" and "Burglary" for analysis. 

```{r}
## For instance: 2010 Crime Data import:
## data2010<- data2010 %>% fill(State) %>% fill(Area) %>% filter(Area == 'State Total') %>% 
## filter(...3== 'Rate per 100,000 inhabitants') %>% select(-Population,-...3)
## data2010 <- data2010[, colSums(is.na(data2010)) < nrow(data2010)]

## Crime data from 2000-2003 use different formats:
## data2003 <- read_csv(here::here("dataset", "2003.csv"),skip = 4)%>% 
##   filter(Area!= 'Metropolitan Statistical Area'&
##           Area!='Area actually reporting'&
##           Area!='Estimated totals'&
##           Area!='Cities outside metropolitan areas'&
##           Area!='Rural'&
##           Area!='State Total'&
##           Area!='Total'&
##           Area!='Nonmetropolitan counties'&
##           Area!='Estimated total') %>% 
##   select(c(Area,Robbery,Burglary))
## data2003$Area <- gsub('Rate per 100,000 inhabitants', NA, data2003$Area) 
## data2003 <- data2003 %>% 
##   fill(Area) %>%
##   drop_na(Robbery, Burglary)
```

Then we merged crime data from 2000-2007 and 2010-2017 into two datasets, each dataset is composed of state name and average crime cases. This is because this data is collected when these people were at age 16 and 26 and they were born between between 1984-1992. So we have calculated the time period of these people at age 16 and 26 to seperate the datasets. Note that crime data for year 2004 is missing on the website so we skiped that year during the analysis(since we are using the average it should not be an issue).

```{r}
## Combine 2000-2007 crime data
## df_list <- list(data2000, data2001, data2002,data2003,data2005,data2006,data2007)
## mergedf <- Reduce(function(x, y) merge(x, y, all=TRUE), df_list)
## mergedf$Area <- gsub('[0-9., ]', '', mergedf$Area)
## mergedf <- mergedf %>%
##   group_by(Area) %>%
##   summarize(meanRobbery = mean(as.numeric(Robbery)),meanBurglary = mean(Burglary))
```

And the merged dataset is now ready for analysis. 

link to [load_and_clean_data.R](here::here("load_and_clean_data.R"))


## Files in static

### `load_and_clean_data.R`

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/load_and_clean_data.R)`. 


When you are loading in data, I recommend using the `here::here` function to specify the file path. This function is used to specify a path relative to your project's root directory. Hence, you can read a file using eg, `read_csv(here::here("dataset/data_file.csv"))`.


### Shiny Interactive

We have not developed the Shiny interactive yet but we have our ideas listed below:

1. Firstly we want to use one plot graph to show the relationship between crime rate and income level in each state. Specifically, we will use the majority income level to represent each state (for example, if the population of income level in Q1 is the biggest in the state, then A will be shown as Q1 income level). Most people will assume that the less the income level, the more crime rate. However, there is no significant relationship between those two variables
2. Then, we will use the crosstalk function to make users see the crime rate and migration rate in each state. Users can playing with the selection button on the map to filter different regions among five regions: Northeast, Midwest, South, and West; as well as coastal and noncoastal. Users can also choose the migration rate range in the Magnitude to find corresponding states
3. Finally, if users are interested in predicting their future living location, they can input their personal information on 16-years-old: state, income level, and race, and the website will return the prediction result. 
Or if users are elder than 26, they can also use this to check whether they follow the trend of migration. If wrong, we will ask users whether to provide their informations to help us improve our model.




----

## Rubric: On this page (kept for reference)

you will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.